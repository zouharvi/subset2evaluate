{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import irt_mt_dev.utils as utils\n",
    "import numpy as np\n",
    "import os\n",
    "import subset2evaluate.evaluate\n",
    "import subset2evaluate.select_subset\n",
    "import tqdm\n",
    "\n",
    "os.chdir(\"/home/vilda/irt-mt-dev\")\n",
    "\n",
    "def benchmark_method(repetitions=10, kwargs_dict={}):\n",
    "\tdata_old_all = list(utils.load_data_wmt_all(normalize=True).values())[:9]\n",
    "\tpoints_y_acc = []\n",
    "\tpoints_y_clu = []\n",
    "\n",
    "\t# run multiple times to smooth variance\n",
    "\tfor data_old in data_old_all:\n",
    "\t\tfor _ in range(repetitions):\n",
    "\t\t\twhile True:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\t(_, clu_new), acc_new = subset2evaluate.evaluate.run_evaluate_topk(\n",
    "\t\t\t\t\t\tdata_old,\n",
    "\t\t\t\t\t\tsubset2evaluate.select_subset.run_select_subset(data_old, **kwargs_dict),\n",
    "\t\t\t\t\t\tmetric=\"human\"\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(e)\n",
    "\t\t\t\t\tpass\n",
    "\t\t\tpoints_y_acc.append(acc_new)\n",
    "\t\t\tpoints_y_clu.append(clu_new)\n",
    "\n",
    "\t\t# print(f\"- ACC: {np.average(acc_new):.2%} | CLU: {np.average(clu_new):.2f}\")\n",
    "\tprint(f\"ACC: {np.average(points_y_acc):.2%} | CLU: {np.average(points_y_clu):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random\")\n",
    "benchmark_method(repetitions=10, kwargs_dict={\"method\": \"random\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MetricX-23 avg\")\n",
    "benchmark_method(repetitions=1, kwargs_dict={\"method\": \"avg\", \"metric\": \"MetricX-23\"})\n",
    "print(\"MetricX-23 var\")\n",
    "benchmark_method(repetitions=1, kwargs_dict={\"method\": \"var\", \"metric\": \"MetricX-23\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done with early stopping\n",
    "print(\"IRT Fisher Information Content\")\n",
    "benchmark_method(repetitions=5, kwargs_dict={\"method\": \"irt_fic\", \"metric\": \"MetricX-23\", \"model\": \"scalar\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyIRT Fisher Information Content\")\n",
    "\n",
    "# config = py_irt.config.IrtConfig(\n",
    "#     model_type='4pl',\n",
    "#     log_every=100,\n",
    "#     # dropout=0,\n",
    "#     seed=0,\n",
    "#     # TODO: see what this does\n",
    "#     deterministic=True,\n",
    "# )\n",
    "\n",
    "benchmark_method(repetitions=3, kwargs_dict={\"method\": \"pyirt_fic\", \"metric\": \"MetricX-23\", 'deterministic': True, 'epochs': 1000, 'model_type': '4pl', 'dropout': 0.5, 'priors': 'vague'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
